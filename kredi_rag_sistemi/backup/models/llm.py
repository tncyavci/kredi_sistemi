from llama_cpp import Llama
import os
import warnings
from typing import Dict, List, Optional, Any

# Warnings'leri sustur
warnings.filterwarnings('ignore')
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class MistralLLM:
    """
    Mistral 7B modeli kullanan LLM sÄ±nÄ±fÄ± - CPU Optimized Version
    Bu sÄ±nÄ±f, CPU-only modda Ã§alÄ±ÅŸan Mistral 7B modeli Ã¼zerinden
    kredi uygulamalarÄ± iÃ§in RAG sisteminde kullanÄ±lacak LLM iÅŸlevlerini saÄŸlar.
    """
    
    def __init__(
        self,
        model_path: str = None,
        n_ctx: int = 2048,          # Reduced context for CPU
        n_gpu_layers: int = 0,      # Force CPU-only
        verbose: bool = False,
        temperature: float = 0.7,
        max_tokens: int = 512,      # Reduced for CPU performance
        n_threads: int = 4,         # Optimal for most CPUs
        n_batch: int = 128,         # Smaller batch for CPU
    ):
        """
        MistralLLM sÄ±nÄ±fÄ±nÄ±n baÅŸlatÄ±cÄ±sÄ± - CPU Optimized
        
        Args:
            model_path: GGUF model dosyasÄ±nÄ±n yolu
            n_ctx: Maksimum baÄŸlam uzunluÄŸu (CPU iÃ§in azaltÄ±ldÄ±)
            n_gpu_layers: GPU katman sayÄ±sÄ± (0 = CPU-only)
            verbose: AyrÄ±ntÄ±lÄ± loglama etkinleÅŸtirilsin mi
            temperature: YanÄ±t oluÅŸturma sÄ±caklÄ±ÄŸÄ± (0-1 arasÄ±)
            max_tokens: YanÄ±tta oluÅŸturulacak maksimum token sayÄ±sÄ±
            n_threads: CPU thread sayÄ±sÄ±
            n_batch: Batch boyutu (CPU iÃ§in kÃ¼Ã§Ã¼k)
        """
        self.model_path = model_path or os.getenv("MISTRAL_MODEL_PATH")
        if not self.model_path:
            raise ValueError("Model yolu belirtilmeli veya MISTRAL_MODEL_PATH ortam deÄŸiÅŸkeni ayarlanmalÄ±dÄ±r.")
        
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # CPU-only environment'Ä± zorla
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        os.environ["FORCE_CPU"] = "1"
        
        print(f"ğŸ”„ Mistral model yÃ¼kleniyor (CPU-only mode)...")
        print(f"ğŸ“ Model: {os.path.basename(self.model_path)}")
        print(f"ğŸ’¾ Context: {n_ctx} tokens")
        print(f"ğŸ§µ Threads: {n_threads}")
        print(f"ğŸ“¦ Batch: {n_batch}")
        
        try:
            # Model yÃ¼klemesi - CPU optimized
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=n_ctx,
                n_gpu_layers=0,         # Force CPU-only
                verbose=verbose,
                n_threads=n_threads,    # CPU thread count
                n_batch=n_batch,       # Batch size for CPU
                use_mmap=True,         # Memory mapping for efficiency
                use_mlock=False,       # Don't lock memory
                low_vram=True          # Optimize for low memory
            )
            print("âœ… Mistral model baÅŸarÄ±yla yÃ¼klendi (CPU-only)")
            
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print("ğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri:")
            print("- Model dosyasÄ±nÄ±n mevcut olduÄŸunu kontrol edin")
            print("- Yeterli RAM'iniz olduÄŸunu kontrol edin (minimum 6GB)")
            print("- BaÅŸka programlarÄ± kapatmayÄ± deneyin")
            raise
        
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Verilen prompt iÃ§in metni oluÅŸturur - CPU Optimized
        
        Args:
            prompt: KullanÄ±cÄ± promptu
            system_prompt: Sistem promptu (isteÄŸe baÄŸlÄ±)
            
        Returns:
            OluÅŸturulan yanÄ±t metni
        """
        try:
            if system_prompt:
                # Mistral formatÄ±nda system ve user prompt birleÅŸtirme
                full_prompt = f"[INST] {system_prompt}\n\n{prompt} [/INST]"
            else:
                full_prompt = f"[INST] {prompt} [/INST]"
            
            # CPU iÃ§in optimize edilmiÅŸ generation
            response = self.llm.create_completion(
                prompt=full_prompt,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                echo=False,
                stop=["[INST]", "</s>", "\n\n"],
                top_p=0.9,              # Nucleus sampling
                repeat_penalty=1.1,     # Prevent repetition
                stream=False            # Don't stream for simplicity
            )
            
            return response["choices"][0]["text"].strip()
            
        except Exception as e:
            print(f"âŒ Text generation hatasÄ±: {e}")
            return f"YanÄ±t oluÅŸturulamadÄ±: {str(e)}"
    
    def generate_with_context(self, prompt: str, context: List[str], system_prompt: Optional[str] = None) -> str:
        """
        Verilen baÄŸlam bilgileriyle birlikte yanÄ±t oluÅŸturur (RAG iÃ§in) - Context window optimized
        
        Args:
            prompt: KullanÄ±cÄ± promptu
            context: Belge vektÃ¶r tabanÄ±ndan alÄ±nan relevan dÃ¶kÃ¼manlarÄ±n listesi
            system_prompt: Sistem promptu (isteÄŸe baÄŸlÄ±)
            
        Returns:
            OluÅŸturulan yanÄ±t metni
        """
        # Context'i Ã§ok agresif ÅŸekilde kÄ±sÄ±tla (CPU ve context window iÃ§in)
        limited_context = []
        has_table_data = False
        
        # Maksimum 2 belge, belge baÅŸÄ±na 300 karakter
        max_docs = min(2, len(context))
        max_chars_per_doc = 300
        
        for i, doc in enumerate(context[:max_docs]):
            if doc and len(doc.strip()) > 0:
                # Tablo verisi kontrolÃ¼
                if any(keyword in doc.upper() for keyword in ["TABLO", "TABLE", "|", "2021", "2022", "2023", "2024"]):
                    has_table_data = True
                    max_chars_per_doc = 400  # Tablo iÃ§in biraz daha fazla
                
                # Sadece en Ã¶nemli kÄ±smÄ± al
                truncated_doc = doc.strip()[:max_chars_per_doc]
                
                # CÃ¼mle ortasÄ±nda kesilmeyi Ã¶nle
                if len(truncated_doc) == max_chars_per_doc and '.' in truncated_doc:
                    last_period = truncated_doc.rfind('.')
                    if last_period > max_chars_per_doc // 2:  # En az yarÄ±sÄ± kalmalÄ±
                        truncated_doc = truncated_doc[:last_period + 1]
                
                limited_context.append(truncated_doc)
        
        # Context'i birleÅŸtir
        context_text = "\n\n".join(limited_context) if limited_context else "Ä°lgili bilgi bulunamadÄ±."
        
        # Tablo verisi varsa Ã¶zel sistem promptu
        if has_table_data:
            system_prompt = """Sen TÃ¼rkÃ§e finansal asistansÄ±n. Tablolardaki sayÄ±larÄ± DÄ°KKATLE oku. 
Sadece tabloda gÃ¶rdÃ¼ÄŸÃ¼n sayÄ±larÄ± kullan. Uydurmaca yapma."""
        elif system_prompt is None:
            system_prompt = """Sen TÃ¼rkÃ§e finansal asistansÄ±n. Verilen belgelerdeki bilgileri kullan. 
Belgede bilgi yoksa 'Bu bilgi belgelerde mevcut deÄŸil' de."""
        
        # Prompt'u kÄ±salt
        short_prompt = prompt[:150] if len(prompt) > 150 else prompt
        
        # Final prompt - Ã§ok kÄ±sa ve net
        if has_table_data:
            final_prompt = f"""Sistem: {system_prompt}

Tablo Verileri:
{context_text}

Soru: {short_prompt}

YanÄ±t (sadece tablodaki sayÄ±larÄ± kullan):"""
        else:
            final_prompt = f"""Sistem: {system_prompt}

Belgeler:
{context_text}

Soru: {short_prompt}

YanÄ±t:"""
        
        # Token sayÄ±sÄ±nÄ± kontrol et
        estimated_tokens = len(final_prompt.split()) * 1.3  # Rough estimation
        
        if estimated_tokens > 1400:  # Daha gÃ¼venli limit
            # Daha da kÄ±salt
            context_text = context_text[:300] + "..."
            final_prompt = f"""Belgeler: {context_text}

Soru: {short_prompt}

YanÄ±t:"""
        
        try:
            # LLM'den yanÄ±t al - Ã§ok dÃ¼ÅŸÃ¼k max_tokens
            response = self.llm(
                final_prompt,
                max_tokens=150,  # Daha da dÃ¼ÅŸÃ¼k
                temperature=0.05,  # Daha deterministik
                stop=["Soru:", "Belgeler:", "Sistem:", "Tablo"],
                echo=False
            )
            
            if response and 'choices' in response and len(response['choices']) > 0:
                answer = response['choices'][0]['text'].strip()
                
                # BoÅŸ veya Ã§ok kÄ±sa yanÄ±tlarÄ± kontrol et
                if not answer or len(answer) < 5:
                    return "Bu konuda yeterli bilgi bulamadÄ±m."
                
                # Ä°ngilizce yanÄ±tlarÄ± TÃ¼rkÃ§e'ye Ã§evir (basit kontrol)
                if any(word in answer.lower() for word in ['the', 'and', 'for', 'you', 'would', 'need', 'from']):
                    return "Bu bilgi belgelerde mevcut deÄŸil."
                
                # Ã‡ok uzun yanÄ±tlarÄ± kÄ±salt
                if len(answer) > 300:
                    answer = answer[:300] + "..."
                
                return answer
            else:
                return "YanÄ±t oluÅŸturulamadÄ±."
                
        except Exception as e:
            print(f"âŒ Text generation hatasÄ±: {e}")
            return f"YanÄ±t oluÅŸturulamadÄ±: {str(e)}"

# Model indirme ve hazÄ±rlÄ±k fonksiyonu - CPU Optimized
def download_mistral_model(model_name: str = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF", 
                          file_name: str = "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
                          save_dir: str = "./models/") -> str:
    """
    Hugging Face'den Mistral 7B modelini indirir (eÄŸer model zaten mevcut deÄŸilse).
    CPU iÃ§in optimize edilmiÅŸ versiyon.
    
    Args:
        model_name: Hugging Face'deki model repository adÄ±
        file_name: Ä°ndirilecek GGUF dosyasÄ±nÄ±n adÄ± (Q4_K_M CPU iÃ§in optimal)
        save_dir: Modelin kaydedileceÄŸi dizin
        
    Returns:
        Ä°ndirilen model dosyasÄ±nÄ±n tam yolu
    """
    import os
    
    try:
        from huggingface_hub import hf_hub_download
    except ImportError:
        print("âŒ huggingface_hub paketi bulunamadÄ±!")
        print("ğŸ’¡ Ã‡Ã¶zÃ¼m: pip install huggingface_hub")
        raise
    
    # Dizini oluÅŸtur
    os.makedirs(save_dir, exist_ok=True)
    
    # Modelin tam yolunu oluÅŸtur
    target_path = os.path.join(save_dir, file_name)
    
    # Model zaten var mÄ± kontrol et
    if os.path.exists(target_path):
        print(f"âœ… Model zaten mevcut: {target_path}")
        file_size_mb = os.path.getsize(target_path) / (1024 * 1024)
        print(f"ğŸ“Š Dosya boyutu: {file_size_mb:.1f} MB")
        return target_path
    
    # Model yoksa indir
    print(f"ğŸ“¥ Model indiriliyor: {model_name}/{file_name}")
    print("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
    
    try:
        model_path = hf_hub_download(
            repo_id=model_name,
            filename=file_name,
            local_dir=save_dir,
            local_dir_use_symlinks=False  # Symlink kullanma
        )
        
        print(f"âœ… Model baÅŸarÄ±yla indirildi: {model_path}")
        file_size_mb = os.path.getsize(model_path) / (1024 * 1024)
        print(f"ğŸ“Š Dosya boyutu: {file_size_mb:.1f} MB")
        return model_path
        
    except Exception as e:
        print(f"âŒ Model indirme hatasÄ±: {e}")
        print("ğŸ’¡ Ã‡Ã¶zÃ¼m Ã¶nerileri:")
        print("- Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin")
        print("- Disk alanÄ±nÄ±zÄ± kontrol edin (minimum 5GB)")
        print("- Proxy/firewall ayarlarÄ±nÄ±zÄ± kontrol edin")
        raise

def get_model_info(model_path: str) -> Dict[str, Any]:
    """
    Model hakkÄ±nda bilgi dÃ¶ndÃ¼rÃ¼r
    
    Args:
        model_path: Model dosyasÄ±nÄ±n yolu
        
    Returns:
        Model bilgileri
    """
    if not os.path.exists(model_path):
        return {"exists": False}
    
    file_size_mb = os.path.getsize(model_path) / (1024 * 1024)
    
    return {
        "exists": True,
        "path": model_path,
        "filename": os.path.basename(model_path),
        "size_mb": round(file_size_mb, 1),
        "size_gb": round(file_size_mb / 1024, 2)
    }