"""
Geli≈ümi≈ü PDF i≈üleme mod√ºl√º.
OCR, tablo tanƒ±ma ve yapƒ±landƒ±rƒ±lmƒ±≈ü veri √ßƒ±karma √∂zellikleriyle donatƒ±lmƒ±≈ü
PDF i≈üleme i≈ülevleri saƒülar.
"""

import os
import re
import logging
import tempfile
import gc
import psutil
import threading
import time
from typing import List, Dict, Any, Optional, Tuple, Union, Generator, Iterator
from pathlib import Path
import json
from functools import wraps
from queue import Queue
from concurrent.futures import ThreadPoolExecutor

# PDF i≈üleme k√ºt√ºphaneleri
import PyPDF2
import pdfplumber
import fitz  # PyMuPDF
import camelot
import tabula
import pandas as pd
import numpy as np

# OCR i√ßin k√ºt√ºphaneler
import pytesseract
from pdf2image import convert_from_path
import cv2

# GPU kontrol√º i√ßin
import torch

from .text_stream_processor import TextStreamProcessor
from .financial_table_processor import FinancialTableProcessor

logger = logging.getLogger(__name__)

# Bellek izleme ve temizleme i√ßin yardƒ±mcƒ± fonksiyonlar
def get_memory_usage() -> float:
    """Mevcut i≈ülemin bellek kullanƒ±mƒ±nƒ± MB cinsinden d√∂nd√ºr√ºr"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    memory_mb = mem_info.rss / (1024 * 1024)
    return memory_mb

def memory_monitor(threshold_mb: float = 1000, check_interval: float = 5.0):
    """Bellek kullanƒ±mƒ±nƒ± izleyen dekorat√∂r"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ƒ∞zleme i√ßin thread olu≈ütur
            stop_event = threading.Event()
            
            def monitor_memory():
                while not stop_event.is_set():
                    mem_usage = get_memory_usage()
                    if mem_usage > threshold_mb:
                        logger.warning(f"Y√ºksek bellek kullanƒ±mƒ±: {mem_usage:.2f} MB. GC √ßaƒürƒ±lƒ±yor.")
                        gc.collect()
                    stop_event.wait(check_interval)
            
            # ƒ∞zleme thread'ini ba≈ülat
            monitor_thread = threading.Thread(target=monitor_memory, daemon=True)
            monitor_thread.start()
            
            try:
                # Asƒ±l fonksiyonu √ßalƒ±≈ütƒ±r
                result = func(*args, **kwargs)
                return result
            finally:
                # ƒ∞zleme thread'ini durdur
                stop_event.set()
                gc.collect()
        
        return wrapper
    return decorator

def force_gc_after(func):
    """Bir fonksiyondan sonra garbage collection'ƒ± zorlayan dekorat√∂r"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)
        gc.collect()
        return result
    return wrapper

class EnhancedPdfProcessor:
    """
    Geli≈ümi≈ü PDF i≈üleme yetenekleri sunan sƒ±nƒ±f.
    Bu sƒ±nƒ±f, OCR ve tablo tanƒ±ma dahil olmak √ºzere √ße≈üitli PDF i≈üleme i≈ülevleri saƒülar.
    """
    
    def __init__(
        self,
        output_dir: str = "./data/processed",
        ocr_lang: str = "tur+eng",  # OCR dil se√ßeneƒüi (T√ºrk√ße + ƒ∞ngilizce)
        ocr_threshold: float = 0.7,  # OCR g√ºven e≈üiƒüi
        table_extraction_method: str = "auto",  # 'camelot', 'tabula' veya 'auto'
        memory_threshold_mb: float = 1000,  # Bellek temizleme e≈üiƒüi (MB)
        use_streaming: bool = True,  # Akƒ±≈ü tabanlƒ± i≈üleme kullanƒ±lsƒ±n mƒ±
        max_workers: int = 4,  # Paralel i≈üleme i√ßin maksimum i≈ü par√ßacƒ±ƒüƒ± sayƒ±sƒ±
        use_gpu: bool = None,  # GPU kullanƒ±lsƒ±n mƒ± (None: otomatik tespit)
        gpu_batch_size: int = 4  # GPU i√ßin batch boyutu
    ):
        """
        EnhancedPdfProcessor sƒ±nƒ±fƒ±nƒ±n ba≈ülatƒ±cƒ±sƒ±.
        
        Args:
            output_dir: ƒ∞≈ülenmi≈ü verilerin kaydedileceƒüi dizin
            ocr_lang: OCR i√ßin dil se√ßeneƒüi
            ocr_threshold: OCR sonu√ßlarƒ± i√ßin g√ºven e≈üiƒüi
            table_extraction_method: Tablo √ßƒ±karma y√∂ntemi ('camelot', 'tabula' veya 'auto')
            memory_threshold_mb: Bellek temizleme e≈üiƒüi (MB)
            use_streaming: Akƒ±≈ü tabanlƒ± i≈üleme kullanƒ±lsƒ±n mƒ±
            max_workers: Paralel i≈üleme i√ßin maksimum i≈ü par√ßacƒ±ƒüƒ± sayƒ±sƒ±
            use_gpu: GPU kullanƒ±lsƒ±n mƒ± (None: otomatik tespit)
            gpu_batch_size: GPU i√ßin batch boyutu
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        self.ocr_lang = ocr_lang
        self.ocr_threshold = ocr_threshold
        self.table_extraction_method = table_extraction_method
        self.memory_threshold_mb = memory_threshold_mb
        self.use_streaming = use_streaming
        self.max_workers = max_workers
        self.gpu_batch_size = gpu_batch_size
        
        # GPU kullanƒ±m durumunu kontrol et
        if use_gpu is None:
            # CUDA veya MPS varsa GPU'yu otomatik olarak etkinle≈ütir
            self.use_gpu = torch.cuda.is_available() or hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
            if self.use_gpu:
                logger.info(f"GPU otomatik olarak tespit edildi. Kullanƒ±labilir: {self.use_gpu}")
                if torch.cuda.is_available():
                    self.device = torch.device("cuda")
                    logger.info(f"CUDA cihazƒ± kullanƒ±lƒ±yor: {torch.cuda.get_device_name(0)}")
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.device = torch.device("mps")
                    logger.info("Apple M1/M2 MPS cihazƒ± kullanƒ±lƒ±yor")
        else:
            self.use_gpu = use_gpu
            if self.use_gpu:
                if torch.cuda.is_available():
                    self.device = torch.device("cuda")
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.device = torch.device("mps")
                else:
                    logger.warning("GPU istendi ancak kullanƒ±labilir GPU bulunamadƒ±. CPU'ya geri d√∂n√ºl√ºyor.")
                    self.use_gpu = False
                    self.device = torch.device("cpu")
            else:
                self.device = torch.device("cpu")
        
        # GPU kullanƒ±mƒ± hakkƒ±nda bilgi ver
        logger.info(f"GPU kullanƒ±mƒ±: {'Aktif' if self.use_gpu else 'Devre dƒ±≈üƒ±'}, Cihaz: {self.device}")
        
        # Initialize financial table processor
        self.financial_processor = FinancialTableProcessor()
        
        self.text_cleaners = [
            self._remove_extra_spaces,
            self._remove_special_characters,
            self._normalize_whitespace
        ]
        
        # Tesseract OCR'ƒ±n y√ºkl√º olduƒüunu kontrol et
        try:
            pytesseract.get_tesseract_version()
            self.ocr_available = True
            logger.info("Tesseract OCR bulundu ve kullanƒ±labilir.")
        except Exception as e:
            self.ocr_available = False
            logger.warning(f"Tesseract OCR kullanƒ±lamƒ±yor: {str(e)}")
            
        # GPU destekli OCR i√ßin EasyOCR'ƒ± kontrol et
        self.easyocr_available = False
        if self.use_gpu:
            try:
                import easyocr
                self.easyocr_reader = easyocr.Reader(['tr', 'en'])
                self.easyocr_available = True
                logger.info("EasyOCR GPU desteƒüi ile kullanƒ±labilir.")
            except ImportError:
                logger.warning("EasyOCR bulunamadƒ±, GPU destekli OCR devre dƒ±≈üƒ±.")
                self.easyocr_available = False
    
    def _check_memory_usage(self) -> bool:
        """
        Bellek kullanƒ±mƒ±nƒ± kontrol eder ve e≈üik deƒüeri a≈üƒ±lmƒ±≈üsa GC √ßaƒüƒ±rƒ±r.
        
        Returns:
            Bellek temizleme i≈ülemi ger√ßekle≈ütiyse True, yoksa False
        """
        memory_mb = get_memory_usage()
        if memory_mb > self.memory_threshold_mb:
            logger.warning(f"Bellek kullanƒ±mƒ± e≈üiƒüi a≈üƒ±ldƒ±: {memory_mb:.2f} MB. Bellek temizleniyor...")
            gc.collect()
            return True
        return False
    
    @staticmethod
    def _validate_pdf_file(file_path: str) -> bool:
        """
        PDF dosyasƒ±nƒ±n ge√ßerli olduƒüunu doƒürular.
        
        Args:
            file_path: PDF dosya yolu
            
        Returns:
            Dosya ge√ßerli bir PDF ise True, deƒüilse False
        """
        if not os.path.exists(file_path):
            logger.warning(f"Dosya bulunamadƒ±: {file_path}")
            return False
        
        if not file_path.lower().endswith('.pdf'):
            logger.warning(f"Ge√ßersiz dosya uzantƒ±sƒ±: {file_path}")
            return False
        
        try:
            # Dosya boyutunu kontrol et
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            max_file_size_mb = 500  # 500MB √ºst sƒ±nƒ±r (daha √∂nce 100MB idi)
            
            if file_size_mb > max_file_size_mb:
                logger.warning(f"PDF dosyasƒ± √ßok b√ºy√ºk: {file_path} ({file_size_mb:.2f} MB), maksimum: {max_file_size_mb} MB")
                return False
                
            logger.info(f"PDF dosya boyutu: {file_size_mb:.2f} MB")
            
            # PDF'in a√ßƒ±labilirliƒüini kontrol et
            with open(file_path, 'rb') as f:
                # PyPDF2 kullanarak PDF'in ge√ßerliliƒüini kontrol et
                try:
                    pdf = PyPDF2.PdfReader(f)
                    # Sayfa sayƒ±sƒ±nƒ± kontrol et (√ßok fazla sayfa varsa uyarƒ± ver)
                    page_count = len(pdf.pages)
                    if page_count > 5000:
                        logger.warning(f"PDF dosyasƒ± √ßok fazla sayfa i√ßeriyor: {file_path} ({page_count} sayfa)")
                        return False
                        
                    logger.info(f"PDF sayfa sayƒ±sƒ±: {page_count}")
                    
                    # PDF'in ≈üifreli olup olmadƒ±ƒüƒ±nƒ± kontrol et
                    if pdf.is_encrypted:
                        logger.warning(f"PDF dosyasƒ± ≈üifreli: {file_path}")
                        return False
                    
                    return True
                except Exception as e:
                    # PyPDF2 ba≈üarƒ±sƒ±z olursa PyMuPDF ile dene
                    logger.warning(f"PyPDF2 ile doƒürulama ba≈üarƒ±sƒ±z, PyMuPDF deneniyor: {str(e)}")
                    
            # PyMuPDF ile dene
            try:
                doc = fitz.open(file_path)
                page_count = doc.page_count
                
                if page_count > 5000:
                    logger.warning(f"PDF dosyasƒ± √ßok fazla sayfa i√ßeriyor: {file_path} ({page_count} sayfa)")
                    doc.close()
                    return False
                
                # ≈ûifreli mi kontrol et
                if doc.isEncrypted:
                    logger.warning(f"PDF dosyasƒ± ≈üifreli: {file_path}")
                    doc.close()
                    return False
                
                doc.close()
                return True
            except Exception as e:
                logger.error(f"PDF doƒürulama hatasƒ± (PyMuPDF): {str(e)}")
                return False
                
        except Exception as e:
            logger.error(f"PDF doƒürulama hatasƒ±: {file_path}, {str(e)}")
            return False
    
    def _remove_extra_spaces(self, text: str) -> str:
        """Fazla bo≈üluklarƒ± temizler"""
        return re.sub(r'\s+', ' ', text).strip()
    
    def _remove_special_characters(self, text: str) -> str:
        """√ñzel karakterleri temizler"""
        # T√ºrk√ße karakterleri koru - basitle≈ütirilmi≈ü regex
        pattern = r'[^\w\s√ß√áƒüƒûƒ±ƒ∞√∂√ñ≈ü≈û√º√ú.,;:!?()%&$#@+\-/*="\']'
        return re.sub(pattern, '', text)
    
    def _normalize_whitespace(self, text: str) -> str:
        """Bo≈üluklarƒ± normalize eder"""
        return re.sub(r'\s+', ' ', text.replace('\n', ' ')).strip()
    
    def clean_text(self, text: str) -> str:
        """
        Metni temizler ve normalize eder.
        
        Args:
            text: Temizlenecek metin
            
        Returns:
            Temizlenmi≈ü metin
        """
        for cleaner in self.text_cleaners:
            text = cleaner(text)
        return text
    
    def extract_text_with_pdfplumber(self, pdf_path: str, max_pages: Optional[int] = None) -> str:
        """
        PDF'ten pdfplumber kullanarak metin √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            max_pages: ƒ∞≈ülenecek maksimum sayfa sayƒ±sƒ± (None=t√ºm√º)
            
        Returns:
            PDF'ten √ßƒ±karƒ±lan metin
        """
        text = ""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                pages = pdf.pages[:max_pages] if max_pages else pdf.pages
                for page in pages:
                    text += page.extract_text() or ""
                    text += "\n\n"
        except Exception as e:
            logger.error(f"pdfplumber ile metin √ßƒ±karƒ±rken hata: {str(e)}")
        
        return text
    
    def extract_text_with_pymupdf(self, pdf_path: str, max_pages: Optional[int] = None) -> str:
        """
        PDF'ten PyMuPDF (fitz) kullanarak metin √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            max_pages: ƒ∞≈ülenecek maksimum sayfa sayƒ±sƒ± (None=t√ºm√º)
            
        Returns:
            PDF'ten √ßƒ±karƒ±lan metin
        """
        text = ""
        try:
            doc = fitz.open(pdf_path)
            page_count = doc.page_count
            pages_to_process = min(page_count, max_pages or page_count)
            
            for page_num in range(pages_to_process):
                page = doc.load_page(page_num)
                text += page.get_text()
                text += "\n\n"
            
            doc.close()
        except Exception as e:
            logger.error(f"PyMuPDF ile metin √ßƒ±karƒ±rken hata: {str(e)}")
        
        return text
    
    def perform_gpu_accelerated_ocr(self, images: List[np.ndarray]) -> List[str]:
        """
        G√∂r√ºnt√º listesi √ºzerinde GPU hƒ±zlandƒ±rmalƒ± OCR ger√ßekle≈ütirir
        
        Args:
            images: ƒ∞≈ülenecek g√∂r√ºnt√º dizisi (numpy array formatƒ±nda)
            
        Returns:
            OCR sonucu metinler listesi
        """
        if not self.easyocr_available:
            logger.warning("EasyOCR y√ºkl√º deƒüil, Tesseract'a geri d√∂n√ºl√ºyor")
            return [pytesseract.image_to_string(img, lang=self.ocr_lang) for img in images]
            
        try:
            import easyocr
            
            batch_results = []
            # Batch'ler halinde i≈üle (bellek i√ßin)
            for i in range(0, len(images), self.gpu_batch_size):
                batch = images[i:i+self.gpu_batch_size]
                
                # GPU i√ßin bellek optimizasyonu
                if i % (self.gpu_batch_size * 3) == 0:
                    if hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                
                # Batch OCR i≈ülemi
                if self.use_gpu:
                    with torch.no_grad():  # Gradyan hesaplamalarƒ±nƒ± devre dƒ±≈üƒ± bƒ±rak
                        batch_texts = []
                        for img in batch:
                            # EasyOCR ile OCR i≈ülemi
                            try:
                                result = self.easyocr_reader.readtext(img)
                                text = " ".join([entry[1] for entry in result])
                                batch_texts.append(text)
                            except Exception as e:
                                logger.error(f"GPU OCR hatasƒ±: {str(e)}")
                                # Hata durumunda Tesseract'a geri d√∂n
                                text = pytesseract.image_to_string(img, lang=self.ocr_lang)
                                batch_texts.append(text)
                else:
                    # GPU kullanƒ±lamazsa Tesseract'a geri d√∂n
                    batch_texts = [pytesseract.image_to_string(img, lang=self.ocr_lang) for img in batch]
                    
                batch_results.extend(batch_texts)
                
            return batch_results
            
        except Exception as e:
            logger.error(f"GPU OCR genel hatasƒ±: {str(e)}")
            # Sorun durumunda geleneksel OCR'a geri d√∂n
            return [pytesseract.image_to_string(img, lang=self.ocr_lang) for img in images]

    def preprocess_images_for_ocr(self, images: List[np.ndarray]) -> List[np.ndarray]:
        """
        OCR √∂ncesinde g√∂r√ºnt√ºleri iyile≈ütirmek i√ßin √∂n i≈üleme yapar
        
        Args:
            images: ƒ∞≈ülenecek g√∂r√ºnt√º dizisi
            
        Returns:
            ƒ∞yile≈ütirilmi≈ü g√∂r√ºnt√º dizisi
        """
        processed_images = []
        
        for img in images:
            # Griye d√∂n√º≈üt√ºr
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # G√ºr√ºlt√ºy√º gider (Gaussian blur)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            
            # Kontrastƒ± artƒ±r (CLAHE)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(blurred)
            
            # Adaptif e≈üikleme
            binary = cv2.adaptiveThreshold(
                enhanced,
                255,
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                cv2.THRESH_BINARY_INV,
                11,
                2
            )
            
            # Morfolojik i≈ülemler
            kernel = np.ones((3, 3), np.uint8)
            opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)
            closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=1)
            
            # OCR i√ßin yeniden tersine √ßevir (siyah yazƒ± beyaz zemin)
            final = cv2.bitwise_not(closing)
            
            # Dokumanƒ±n eƒüikliƒüini d√ºzelt
            coords = np.column_stack(np.where(final > 0))
            if len(coords) > 0:
                try:
                    angle = cv2.minAreaRect(coords)[-1]
                    if angle < -45:
                        angle = -(90 + angle)
                    else:
                        angle = -angle
                        
                    # Eƒüer belirgin bir eƒüiklik varsa d√ºzelt
                    if abs(angle) > 0.5:
                        (h, w) = final.shape[:2]
                        center = (w // 2, h // 2)
                        M = cv2.getRotationMatrix2D(center, angle, 1.0)
                        final = cv2.warpAffine(final, M, (w, h), 
                                              flags=cv2.INTER_CUBIC, 
                                              borderMode=cv2.BORDER_REPLICATE)
                except Exception as e:
                    logger.warning(f"G√∂r√ºnt√º eƒüikliƒüi d√ºzeltme hatasƒ±: {str(e)}")
            
            processed_images.append(final)
            
        return processed_images

    def perform_ocr(self, pdf_path: str, page_numbers: Optional[List[int]] = None) -> str:
        """
        PDF √ºzerinde OCR ger√ßekle≈ütirir.
        
        Args:
            pdf_path: OCR yapƒ±lacak PDF dosyasƒ±nƒ±n yolu
            page_numbers: OCR yapƒ±lacak sayfalarƒ±n listesi (None ise t√ºm sayfalar)
            
        Returns:
            OCR i≈ülemi sonucu metin
        """
        if not self.ocr_available and not self.easyocr_available:
            logger.error("OCR i≈ülemi ger√ßekle≈ütirilemiyor: OCR k√ºt√ºphanesi bulunamadƒ±.")
            return ""
            
        try:
            # PDF'i g√∂r√ºnt√ºlere d√∂n√º≈üt√ºr
            logger.info(f"PDF g√∂r√ºnt√ºlere d√∂n√º≈üt√ºr√ºl√ºyor: {pdf_path}")
            images = convert_from_path(pdf_path, 300)  # 300 DPI
            
            # Belirli sayfalar se√ßildiyse sadece o sayfalarƒ± i≈üle
            if page_numbers:
                selected_images = [images[i-1] for i in page_numbers if 0 < i <= len(images)]
                if not selected_images:
                    logger.warning(f"Belirtilen sayfa numaralarƒ± PDF'te bulunamadƒ±: {page_numbers}")
                    selected_images = images
            else:
                selected_images = images
                
            # Bellek verimliliƒüi i√ßin atƒ±k g√∂r√ºnt√ºleri temizle
            if selected_images != images:
                del images
                gc.collect()
            
            logger.info(f"Toplam {len(selected_images)} sayfa OCR i≈ülemi i√ßin hazƒ±rlanƒ±yor")
            
            # G√∂r√ºnt√ºleri numpy array'lere d√∂n√º≈üt√ºr
            np_images = []
            for img in selected_images:
                # PIL g√∂r√ºnt√ºs√ºn√º numpy array'e d√∂n√º≈üt√ºr
                np_img = np.array(img)
                np_images.append(np_img)
                
            # Bellek verimliliƒüi i√ßin orijinal g√∂r√ºnt√ºleri temizle
            del selected_images
            gc.collect()
            
            # G√∂r√ºnt√ºleri iyile≈ütir
            logger.info("G√∂r√ºnt√ºler OCR √∂ncesi iyile≈ütiriliyor")
            processed_images = self.preprocess_images_for_ocr(np_images)
            
            # Bellek verimliliƒüi i√ßin orijinal g√∂r√ºnt√ºleri temizle
            del np_images
            gc.collect()
            
            # OCR i≈ülemini ger√ßekle≈ütir
            ocr_results = []
            
            # GPU destekli OCR kullanƒ±labiliyorsa
            if self.use_gpu and self.easyocr_available:
                logger.info("GPU destekli OCR ba≈ülatƒ±lƒ±yor")
                
                # G√∂r√ºnt√ºleri batches halinde i≈üle
                ocr_texts = self.perform_gpu_accelerated_ocr(processed_images)
                ocr_results.extend(ocr_texts)
            else:
                # Standart Tesseract OCR kullan
                logger.info("Tesseract OCR ba≈ülatƒ±lƒ±yor")
                for img in processed_images:
                    # G√∂r√ºnt√ºy√º OCR ile i≈üle
                    text = pytesseract.image_to_string(img, lang=self.ocr_lang)
                    ocr_results.append(text)
            
            # Bellek temizleme
            del processed_images
            gc.collect()
            
            # Sonu√ßlarƒ± birle≈ütir
            final_text = "\n\n".join(ocr_results)
            
            # Metni temizle
            final_text = self.clean_text(final_text)
            
            logger.info(f"OCR tamamlandƒ±, {len(ocr_results)} sayfa metin √ßƒ±karƒ±ldƒ±")
            
            return final_text
            
        except Exception as e:
            logger.error(f"OCR i≈ülemi sƒ±rasƒ±nda hata: {str(e)}")
            return ""
    
    def extract_tables_with_camelot(self, pdf_path: str, pages: str = "all") -> List[pd.DataFrame]:
        """
        PDF'ten camelot kullanarak tablolarƒ± √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            pages: ƒ∞≈ülenecek sayfalar ('all' veya '1,2,3-5' formatƒ±nda)
            
        Returns:
            √áƒ±karƒ±lan tablolarƒ±n DataFrame listesi
        """
        tables = []
        try:
            # √ñnce lattice (√ßizgili tablolar) dene
            logger.info(f"Camelot lattice ile tablo √ßƒ±karma deneniyor: {pdf_path}")
            extracted_tables = camelot.read_pdf(
                pdf_path,
                pages=pages,
                flavor='lattice',  # √áizgiler ve sƒ±nƒ±rlar olan tablolar i√ßin
                suppress_stdout=True,
                line_scale=40,  # √áizgileri daha iyi tespit etmek i√ßin
                strip_text='\n' # Satƒ±r i√ßi yeni satƒ±rlarƒ± temizle
            )
            
            logger.info(f"Camelot lattice ile {len(extracted_tables)} tablo bulundu.")
            
            # Kaliteli ve kullanƒ±labilir tablolarƒ± filtrele
            for i, table in enumerate(extracted_tables):
                if table.accuracy > 60:  # E≈üiƒüi d√º≈ü√ºrd√ºk
                    tables.append(table.df)
                    
            # Eƒüer hi√ß tablo bulunamazsa ba≈üka flavor dene
            if not tables:
                logger.info(f"Camelot stream ile tablo √ßƒ±karma deneniyor: {pdf_path}")
                # Stream flavor ile dene (√ßizgisiz tablolar)
                extracted_tables = camelot.read_pdf(
                    pdf_path,
                    pages=pages,
                    flavor='stream',  # √áizgisiz tablolar i√ßin
                    suppress_stdout=True,
                    strip_text='\n',
                    edge_tol=500 # Daha esnek kenar tespiti
                )
                
                logger.info(f"Camelot stream ile {len(extracted_tables)} tablo bulundu.")
                
                for i, table in enumerate(extracted_tables):
                    if table.accuracy > 50:  # Stream i√ßin daha d√º≈ü√ºk e≈üik
                        tables.append(table.df)
        except Exception as e:
            logger.error(f"Camelot ile tablo √ßƒ±karƒ±rken hata: {str(e)}")
            
        return tables
    
    def extract_tables_with_tabula(self, pdf_path: str, pages: Union[str, List[int]] = "all") -> List[pd.DataFrame]:
        """
        PDF'ten tabula kullanarak tablolarƒ± √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            pages: ƒ∞≈ülenecek sayfalar ('all' veya sayfa numaralarƒ± listesi)
            
        Returns:
            √áƒ±karƒ±lan tablolarƒ±n DataFrame listesi
        """
        try:
            logger.info(f"Tabula ile tablo √ßƒ±karma deneniyor: {pdf_path}")
            
            # √ñnce automatic detection dene
            tables = tabula.read_pdf(
                pdf_path,
                pages=pages,
                multiple_tables=True
            )
            
            logger.info(f"Tabula ile {len(tables)} tablo bulundu.")
            
            # Eƒüer tablo bulunamazsa, daha agresif detection dene
            if not tables:
                logger.info("Tabula guess=True ile tekrar deneniyor...")
                tables = tabula.read_pdf(
                    pdf_path,
                    pages=pages,
                    multiple_tables=True,
                    guess=True,
                    pandas_options={'header': None}
                )
                logger.info(f"Tabula guess=True ile {len(tables)} tablo bulundu.")
                
            # Bo≈ü tablolar varsa kaldƒ±r
            return [table for table in tables if not table.empty]
        except Exception as e:
            logger.error(f"Tabula ile tablo √ßƒ±karƒ±rken hata: {str(e)}")
            return []
    
    def extract_tables(self, pdf_path: str, pages: str = "all") -> List[pd.DataFrame]:
        """
        En iyi y√∂ntemi kullanarak PDF'ten tablolarƒ± √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            pages: ƒ∞≈ülenecek sayfalar
            
        Returns:
            √áƒ±karƒ±lan tablolarƒ±n DataFrame listesi
        """
        tables = []
        
        if self.table_extraction_method == "camelot":
            tables = self.extract_tables_with_camelot(pdf_path, pages)
        elif self.table_extraction_method == "tabula":
            tables = self.extract_tables_with_tabula(pdf_path, pages)
        else:  # "auto" mod - her iki y√∂ntemi de dene
            # √ñnce camelot dene
            tables = self.extract_tables_with_camelot(pdf_path, pages)
            
            # Eƒüer camelot tablo bulamadƒ±ysa tabula'yƒ± dene
            if not tables:
                tables = self.extract_tables_with_tabula(pdf_path, pages)
        
        logger.info(f"{len(tables)} tablo ba≈üarƒ±yla √ßƒ±karƒ±ldƒ±.")
        return tables
    
    def tables_to_text(self, tables: List[pd.DataFrame]) -> str:
        """
        Tablolarƒ± metin formatƒ±na d√∂n√º≈üt√ºr√ºr.
        NaN deƒüerleri tamamen temizler ve LLM-friendly format √ºretir.
        
        Args:
            tables: DataFrame listesi
            
        Returns:
            Tablolarƒ±n metin g√∂sterimi
        """
        if not tables:
            return ""
            
        text = ""
        for i, table in enumerate(tables):
            # Tablo ba≈ülƒ±k satƒ±rƒ±  
            text += f"\n**TABLO {i+1}**\n"
            
            if table.empty:
                text += "Bo≈ü tablo\n"
                continue
            
            # Make a copy to avoid modifying original
            table_copy = table.copy()
            
            # Remove completely empty rows and columns
            table_copy = table_copy.dropna(how='all')  # Remove rows that are all NaN
            table_copy = table_copy.loc[:, ~table_copy.isna().all()]  # Remove columns that are all NaN
            
            if table_copy.empty:
                text += "Veri i√ßermeyen tablo\n"
                continue
            
            # Clean and identify meaningful columns
            meaningful_columns = []
            meaningful_column_indices = []
            
            for j, col in enumerate(table_copy.columns):
                col_str = str(col).strip()
                # Skip unnamed or empty columns that are mostly NaN
                if (col_str.startswith('Unnamed:') or col_str == 'nan' or not col_str):
                    # Check if this column has any meaningful data
                    column_data = table_copy.iloc[:, j].dropna()
                    if len(column_data) == 0:
                        continue  # Skip completely empty unnamed columns
                
                meaningful_columns.append(col_str if not col_str.startswith('Unnamed:') else "")
                meaningful_column_indices.append(j)
            
            if not meaningful_columns:
                text += "ƒ∞≈ülenebilir s√ºtun bulunamadƒ±\n"
                continue
                
            # Create clean data structure
            clean_rows = []
            
            for idx, row in table_copy.iterrows():
                clean_row = []
                for col_idx in meaningful_column_indices:
                    cell_value = row.iloc[col_idx]
                    
                    if pd.isna(cell_value):
                        clean_row.append("")  # Empty string for NaN
                    elif isinstance(cell_value, (int, float)):
                        # Format numbers nicely
                        if cell_value == int(cell_value):  # Integer
                            clean_row.append(f"{int(cell_value):,}".replace(",", "."))
                        else:  # Float
                            clean_row.append(f"{cell_value:,.2f}".replace(",", "."))
                    else:
                        # Clean string values
                        str_val = str(cell_value).strip().replace("\n", " ")
                        clean_row.append(str_val)
                
                # Only add rows that have meaningful content
                if any(cell.strip() for cell in clean_row):
                    clean_rows.append(clean_row)
            
            if not clean_rows:
                text += "ƒ∞≈ülenebilir veri bulunamadƒ±\n"
                continue
            
            # Build structured table text
            # First, identify the main title/header row if exists
            main_title = ""
            data_start_idx = 0
            
            if clean_rows:
                first_row = clean_rows[0]
                # Check if first row is a title (has text in first column, rest empty)
                if (first_row[0] and 
                    all(not cell.strip() for cell in first_row[1:]) and
                    not any(char.isdigit() for char in first_row[0])):
                    main_title = first_row[0]
                    data_start_idx = 1
            
            if main_title:
                text += f"üìä {main_title}\n\n"
            
            # Process remaining rows for year headers and data
            year_headers = []
            data_rows = []
            
            # Look for year headers (2021, 2022, etc.)
            if data_start_idx < len(clean_rows):
                potential_year_row = clean_rows[data_start_idx]
                years_found = []
                for cell in potential_year_row:
                    if cell and any(year in cell for year in ['2021', '2022', '2023', '2024']):
                        years_found.append(cell)
                
                if years_found:
                    year_headers = years_found
                    data_start_idx += 1
            
            # Process data rows
            for row_idx in range(data_start_idx, len(clean_rows)):
                row = clean_rows[row_idx]
                
                # Skip empty or header-like rows
                if not any(cell.strip() for cell in row):
                    continue
                    
                # Skip currency/info rows
                first_cell = row[0].strip().lower()
                if first_cell in ['sunum para birimi', 'finansal tablo niteliƒüi']:
                    continue
                
                data_rows.append(row)
            
            # Generate final table format
            if year_headers:
                header_line = "A√ßƒ±klama | " + " | ".join(year_headers)
                text += header_line + "\n"
                text += "-" * len(header_line) + "\n"
            
            for row in data_rows:
                if row and row[0].strip():  # Only rows with a description
                    # Format: Description | Value1 | Value2 | ...
                    description = row[0].strip()
                    values = []
                    
                    # Extract numeric values from the row, skipping empty cells
                    for cell in row[1:]:
                        if cell.strip() and cell.strip() != "":
                            values.append(cell.strip())
                    
                    if values:  # Only add rows with actual values
                        row_text = description + " | " + " | ".join(values)
                        text += row_text + "\n"
            
            text += "\n"  # Space between tables
        
        return text
    
    def tables_to_json(self, tables: List[pd.DataFrame], context: str = "") -> List[Dict]:
        """
        Tablolarƒ± JSON formatƒ±na d√∂n√º≈üt√ºr√ºr ve finansal tablo √∂zellikleri ekler.
        
        Args:
            tables: DataFrame listesi
            context: Tablolarƒ±n bulunduƒüu baƒülam metni
            
        Returns:
            Tablolarƒ±n JSON g√∂sterimi (finansal metadata ile zenginle≈ütirilmi≈ü)
        """
        result = []
        for i, table in enumerate(tables):
            # Bo≈ü tablolarƒ± atla
            if table.empty:
                continue
                
            # NaN deƒüerleri bo≈ü stringlerle deƒüi≈ütir
            table = table.fillna("")
            
            # Financial table processing - Yeni!
            try:
                financial_analysis = self.financial_processor.process_financial_table(table, context)
                is_financial = financial_analysis['table_type'] != 'financial_table'
                
                # Enhanced metadata with financial information
                enhanced_metadata = financial_analysis['metadata']
            except Exception as e:
                logger.warning(f"Financial analysis failed for table {i}: {str(e)}")
                is_financial = False
                enhanced_metadata = {}
                financial_analysis = {}
            
            # Tablo verilerini d√ºzenle
            headers = [str(h).strip() for h in table.columns]
            records = []
            
            for _, row in table.iterrows():
                record = {}
                for j, header in enumerate(headers):
                    # ƒ∞ndeks kullan √ß√ºnk√º bazƒ± ba≈ülƒ±klar bo≈ü olabilir
                    value = row.iloc[j]
                    
                    # Deƒüeri uygun formata d√∂n√º≈üt√ºr
                    if isinstance(value, (float, int)) and not pd.isna(value):
                        # Sayƒ±sal deƒüerleri d√ºzg√ºn formatla
                        value = float(value) if isinstance(value, float) else int(value)
                    elif isinstance(value, str):
                        # Metin deƒüerlerini temizle
                        value = value.strip().replace("\n", " ")
                    else:
                        value = str(value)
                        
                    # Bo≈ü ba≈ülƒ±k durumunu ele al
                    if not header:
                        header = f"Column_{j+1}"
                        
                    record[header] = value
                    
                records.append(record)
            
            # Create enhanced table info with financial metadata
            table_info = {
                "table_id": i+1,
                "table_name": f"Tablo_{i+1}",
                "headers": headers,
                "row_count": len(records),
                "column_count": len(headers),
                "data": records,
                
                # Financial enhancements
                "is_financial_table": is_financial,
                "financial_metadata": enhanced_metadata,
                "financial_analysis": financial_analysis.get('financial_data', {}),
                "table_type": financial_analysis.get('table_type', 'general'),
                "data_quality_score": enhanced_metadata.get('data_quality_score', 0.0)
            }
            
            result.append(table_info)
            
        return result
        
    def extract_metadata(self, pdf_path: str) -> Dict[str, Any]:
        """
        PDF'in meta verilerini √ßƒ±karƒ±r.
        
        Args:
            pdf_path: PDF dosya yolu
            
        Returns:
            PDF meta verileri
        """
        metadata = {
            "filename": os.path.basename(pdf_path),
            "file_size_kb": os.path.getsize(pdf_path) / 1024,
            "created_at": None,
            "modified_at": None,
            "author": None,
            "title": None,
            "subject": None,
            "keywords": None,
            "total_pages": 0
        }
        
        try:
            # PyPDF2 ile meta verileri √ßƒ±kar
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                metadata["total_pages"] = len(reader.pages)
                
                if reader.metadata:
                    info = reader.metadata
                    metadata["author"] = info.get('/Author')
                    metadata["title"] = info.get('/Title')
                    metadata["subject"] = info.get('/Subject')
                    metadata["keywords"] = info.get('/Keywords')
                    
                    # Tarihler
                    if info.get('/CreationDate'):
                        metadata["created_at"] = str(info.get('/CreationDate'))
                    if info.get('/ModDate'):
                        metadata["modified_at"] = str(info.get('/ModDate'))
                        
            # PyMuPDF ile daha fazla detay
            try:
                doc = fitz.open(pdf_path)
                # Sayfa boyutlarƒ±
                first_page = doc[0]
                metadata["page_size"] = {
                    "width": first_page.rect.width,
                    "height": first_page.rect.height
                }
                doc.close()
            except Exception as e:
                logger.warning(f"PyMuPDF meta verisi √ßƒ±karƒ±lamadƒ±: {str(e)}")
                
        except Exception as e:
            logger.error(f"Meta veri √ßƒ±karƒ±rken hata: {str(e)}")
        
        return metadata
        
    def process_pdf(
        self, 
        pdf_path: str, 
        category: str = "genel", 
        extract_tables: bool = True,
        use_ocr: bool = True,
        max_pages: Optional[int] = None,
        chunk_size: int = 1000,
        overlap: int = 200,
        use_cache: bool = True,
        force_refresh: bool = False,
        cache_manager = None
    ) -> Dict[str, Any]:
        """
        Bir PDF dosyasƒ±nƒ± tam olarak i≈üler.
        
        Args:
            pdf_path: ƒ∞≈ülenecek PDF dosya yolu
            category: Belge kategorisi
            extract_tables: Tablolarƒ± √ßƒ±kar
            use_ocr: OCR uygula
            max_pages: ƒ∞≈ülenecek maksimum sayfa sayƒ±sƒ±
            chunk_size: Metin par√ßalama uzunluƒüu
            overlap: Metin par√ßalarƒ± arasƒ± √∂rt√º≈üme
            use_cache: √ñnbellek kullanƒ±lsƒ±n mƒ±
            force_refresh: √ñnbelleƒüi zorla yenile
            cache_manager: Kullanƒ±lacak √∂nbellek y√∂neticisi
            
        Returns:
            ƒ∞≈ülenmi≈ü PDF verileri
        """
        if not self._validate_pdf_file(pdf_path):
            logger.error(f"Ge√ßersiz PDF dosyasƒ±: {pdf_path}")
            return {}
            
        # √ñnbellekleme i√ßin i≈üleme parametrelerini hazƒ±rla
        process_params = {
            "category": category,
            "extract_tables": extract_tables,
            "use_ocr": use_ocr,
            "max_pages": max_pages,
            "chunk_size": chunk_size,
            "overlap": overlap,
            "use_gpu": self.use_gpu,
            "gpu_batch_size": self.gpu_batch_size
        }
        
        # √ñnbellekten y√ºkleme
        if use_cache and not force_refresh:
            try:
                from utils.preprocessing.pdf_cache_manager import PDFCacheManager
                
                # √ñnbellek y√∂neticisi olu≈ütur veya var olanƒ± kullan
                cache_mgr = cache_manager or PDFCacheManager()
                
                # √ñnbellekten PDF sonucunu al
                cached_result = cache_mgr.get(pdf_path, process_params)
                if cached_result:
                    logger.info(f"PDF dosyasƒ± √∂nbellekten y√ºklendi: {pdf_path}")
                    return cached_result
            except ImportError:
                logger.warning("√ñnbellek y√∂neticisi bulunamadƒ±, √∂nbellekleme atlanƒ±yor")
            except Exception as e:
                logger.warning(f"√ñnbellekten y√ºkleme hatasƒ±: {str(e)}")
        
        result = {
            "id": os.path.splitext(os.path.basename(pdf_path))[0],
            "source": os.path.basename(pdf_path),
            "category": category,
            "metadata": self.extract_metadata(pdf_path),
            "text": "",
            "tables": [],
            "raw_tables": [],
            "table_text": "",
            "chunks": []
        }
        
        # Ba≈ülangƒ±√ß zamanƒ±nƒ± kaydet
        start_time = time.time()
        
        # Metni √ßƒ±kar
        pdf_text = self.extract_text_with_pymupdf(pdf_path, max_pages)
        
        # Metin √ßƒ±kartƒ±lamadƒ±ysa pdfplumber dene
        if not pdf_text.strip():
            pdf_text = self.extract_text_with_pdfplumber(pdf_path, max_pages)
            
        # Hala metin yoksa OCR uygula
        if not pdf_text.strip() and use_ocr:
            logger.info(f"Metinsel i√ßerik bulunamadƒ±, OCR uygulanƒ±yor: {pdf_path}")
            if self.use_gpu and self.easyocr_available:
                logger.info(f"GPU destekli OCR kullanƒ±lƒ±yor: {pdf_path}")
            pdf_text = self.perform_ocr(pdf_path)
        
        # Metni temizle
        pdf_text = self.clean_text(pdf_text)
        result["text"] = pdf_text
        
        # Tablolarƒ± √ßƒ±kar
        if extract_tables:
            tables = self.extract_tables(pdf_path)
            result["tables"] = self.tables_to_json(tables)
            result["raw_tables"] = tables  # DataFrame formatƒ±nda ham tablolar
            result["table_text"] = self.tables_to_text(tables)
        
        # Metni par√ßalara b√∂l
        result["chunks"] = self.chunk_text(pdf_text, chunk_size, overlap)
        
        # Toplam kelime sayƒ±sƒ±
        result["total_words"] = len(pdf_text.split())
        
        # ƒ∞≈üleme s√ºresini hesapla
        processing_time = time.time() - start_time
        result["processing_info"] = {
            "processing_time_seconds": processing_time,
            "use_gpu": self.use_gpu,
            "ocr_used": use_ocr and not pdf_text.strip(),
            "ocr_engine": "easyocr_gpu" if self.use_gpu and self.easyocr_available else "tesseract",
            "tables_extracted": extract_tables,
            "total_tables": len(result["tables"]),
            "total_chunks": len(result["chunks"]),
            "chunk_size": chunk_size,
            "overlap": overlap
        }
        
        # ƒ∞≈ülenmi≈ü sonu√ßlarƒ± kaydet
        output_path = self.output_dir / f"{result['id']}.json"
        
        # JSON serializasyon i√ßin DataFrame'leri ge√ßici olarak kaldƒ±r
        raw_tables_backup = result.pop("raw_tables", [])
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # raw_tables'ƒ± geri ekle (memory'de kalmasƒ± i√ßin)
        result["raw_tables"] = raw_tables_backup
        
        logger.info(f"PDF ba≈üarƒ±yla i≈ülendi ve kaydedildi: {output_path} ({processing_time:.2f} saniye)")
        
        # √ñnbelleƒüe kaydet
        if use_cache:
            try:
                from utils.preprocessing.pdf_cache_manager import PDFCacheManager
                
                # √ñnbellek y√∂neticisi olu≈ütur veya var olanƒ± kullan
                cache_mgr = cache_manager or PDFCacheManager()
                
                # Sonucu √∂nbelleƒüe kaydet
                cache_mgr.set(pdf_path, process_params, result)
            except ImportError:
                logger.warning("√ñnbellek y√∂neticisi bulunamadƒ±, √∂nbellekleme atlanƒ±yor")
            except Exception as e:
                logger.warning(f"√ñnbelleƒüe kaydetme hatasƒ±: {str(e)}")
        
        return result
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict[str, Any]]:
        """
        Metni belirli uzunlukta par√ßalara b√∂ler.
        
        Args:
            text: B√∂l√ºnecek metin
            chunk_size: Maksimum par√ßa uzunluƒüu (kelime sayƒ±sƒ±)
            overlap: Par√ßalar arasƒ± √∂rt√º≈üme (kelime sayƒ±sƒ±)
            
        Returns:
            Metin par√ßalarƒ±nƒ±n listesi
        """
        if not text:
            return []
            
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) + 1 <= chunk_size:
                current_chunk.append(word)
                current_length += len(word) + 1
            else:
                chunk_text = " ".join(current_chunk)
                chunks.append({
                    "text": chunk_text,
                    "length": len(current_chunk),
                    "byte_size": len(chunk_text.encode('utf-8'))
                })
                
                # √ñrt√º≈üme i√ßin son kelimeleri yeni chunk'a ekle
                overlap_words = min(overlap, len(current_chunk))
                current_chunk = current_chunk[-overlap_words:] + [word]
                current_length = sum(len(w) + 1 for w in current_chunk)
        
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append({
                "text": chunk_text,
                "length": len(current_chunk),
                "byte_size": len(chunk_text.encode('utf-8'))
            })
        
        return chunks
    
    def process_pdf_directory(
        self, 
        directory: str, 
        category_mapping: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        Bir dizindeki t√ºm PDF dosyalarƒ±nƒ± i≈üler.
        
        Args:
            directory: PDF dosyalarƒ±nƒ±n bulunduƒüu dizin
            category_mapping: Dosya adƒ± deseni -> kategori e≈üle≈ütirmeleri
            **kwargs: process_pdf'e ge√ßirilecek ek parametreler
            
        Returns:
            ƒ∞≈ülenmi≈ü PDF verilerinin listesi
        """
        results = []
        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]
        
        for pdf_file in sorted(pdf_files):
            pdf_path = os.path.join(directory, pdf_file)
            
            # Kategoriyi belirle
            category = "genel"
            if category_mapping:
                for pattern, cat in category_mapping.items():
                    if pattern.lower() in pdf_file.lower():
                        category = cat
                        break
            
            try:
                # PDF'i i≈üle
                result = self.process_pdf(pdf_path, category=category, **kwargs)
                if result:
                    results.append(result)
            except Exception as e:
                logger.error(f"PDF i≈ülenirken hata: {pdf_file}, Hata: {str(e)}")
                continue
        
        logger.info(f"{len(results)} PDF dosyasƒ± ba≈üarƒ±yla i≈ülendi.")
        return results
    
    @staticmethod
    def process_pdf_to_documents(
        pdf_path: str,
        category: str = "genel",
        source: str = None,
        chunk_size: int = 1000,
        overlap: int = 200,
        max_pages: Optional[int] = None,
        use_ocr: bool = True,
        extract_tables: bool = True,
        use_gpu: bool = None,
        gpu_batch_size: int = 4,
        use_cache: bool = True,
        force_refresh: bool = False,
        table_extraction_method: str = "auto",
        prioritize_tables: bool = False,
        keep_table_context: bool = False
    ) -> List[Dict[str, Any]]:
        """
        PDF'i doƒürudan belge listesine d√∂n√º≈üt√ºr√ºr.
        
        Args:
            pdf_path: PDF dosya yolu
            category: Belge kategorisi
            source: Belge kaynaƒüƒ± (None ise dosya adƒ± kullanƒ±lƒ±r)
            chunk_size: Metin par√ßalama uzunluƒüu
            overlap: Metin par√ßalarƒ± arasƒ± √∂rt√º≈üme
            max_pages: ƒ∞≈ülenecek maksimum sayfa sayƒ±sƒ±
            use_ocr: OCR kullanƒ±lsƒ±n mƒ±
            extract_tables: Tablolar √ßƒ±karƒ±lsƒ±n mƒ±
            use_gpu: GPU kullanƒ±lsƒ±n mƒ± (None ise otomatik tespit)
            gpu_batch_size: GPU i√ßin batch boyutu
            use_cache: √ñnbellek kullanƒ±lsƒ±n mƒ±
            force_refresh: √ñnbelleƒüi zorla yenile
            table_extraction_method: Tablo √ßƒ±karma y√∂ntemi ('auto', 'camelot', 'tabula')
            prioritize_tables: Tablolar metin par√ßalarƒ±ndan √∂nce gelsin mi
            keep_table_context: Tablolarƒ± baƒülamlarƒ±yla birlikte tut
            
        Returns:
            ƒ∞≈ülenmi≈ü belgelerin listesi
        """
        try:
            # √ñnbellekleme i√ßin cache manager olu≈ütur
            from utils.preprocessing.pdf_cache_manager import PDFCacheManager
            cache_manager = PDFCacheManager()
        except ImportError:
            logger.info("√ñnbellek y√∂neticisi bulunamadƒ±, √∂nbellekleme devre dƒ±≈üƒ±.")
            cache_manager = None
            use_cache = False
        
        # GPU destekli i≈ülemci olu≈ütur
        processor = EnhancedPdfProcessor(
            use_gpu=use_gpu,
            gpu_batch_size=gpu_batch_size,
            table_extraction_method=table_extraction_method
        )
        
        if not source:
            source = os.path.basename(pdf_path)
        
        # √ñnbellekleme √∂zelliƒüini belirterek PDF'i i≈üle
        pdf_data = processor.process_pdf(
            pdf_path,
            category=category,
            chunk_size=chunk_size,
            overlap=overlap,
            max_pages=max_pages,
            use_ocr=use_ocr,
            extract_tables=extract_tables,
            use_cache=use_cache,
            force_refresh=force_refresh,
            cache_manager=cache_manager
        )
        
        if not pdf_data:
            return []
        
        # Belgeleri olu≈ütur
        documents = []
        
        # Eƒüer tablolar √∂nceliklendirilecekse, √∂nce tablo belgelerini ekle
        if prioritize_tables and extract_tables:
            # Tablo belgeleri with enhanced financial processing
            for i, table in enumerate(pdf_data.get("tables", [])):
                table_id = f"{pdf_data['id']}_table_{i}"
                
                # Get original table dataframe for financial processing
                tables_dataframes = pdf_data.get("raw_tables", [])
                table_dataframe = tables_dataframes[i] if i < len(tables_dataframes) else None
                
                # Generate enhanced table text with financial analysis
                if table_dataframe is not None:
                    # Use financial processor for enhanced text generation
                    try:
                        # TEMPORARY FIX: Skip financial processor to avoid NaN issues
                        # financial_analysis = processor.financial_processor.process_financial_table(
                        #     table_dataframe, 
                        #     context=' '.join([chunk.get('text', '') for chunk in pdf_data.get('chunks', [])])
                        # )
                        # table_text = financial_analysis.get('text', '')
                        
                        # Use standard table processing instead
                        table_text = processor.tables_to_text([table_dataframe])
                        financial_analysis = {}
                        
                        # If financial text is empty, fallback to standard processing
                        if not table_text.strip():
                            table_text = processor.tables_to_text([table_dataframe])
                    except Exception as e:
                        logger.warning(f"Financial processing failed for table {i}: {str(e)}")
                        table_text = processor.tables_to_text([table_dataframe])
                        financial_analysis = {}
                else:
                    # Fallback: JSON formatƒ±ndan metin olu≈ütur
                    table_text = f"TABLO {i+1}:\n"
                    
                    # Tablo ba≈ülƒ±klarƒ±
                    headers = table.get("headers", [])
                    if headers:
                        table_text += " | ".join(str(h) for h in headers) + "\n"
                        table_text += "-" * (len(" | ".join(str(h) for h in headers))) + "\n"
                    
                    # Tablo verileri
                    for row in table.get("data", []):
                        if isinstance(row, dict):
                            row_values = [str(cell) if cell is not None else "" for cell in row.values()]
                        else:
                            row_values = [str(cell) if cell is not None else "" for cell in row]
                        table_text += " | ".join(row_values) + "\n"
                    
                    financial_analysis = {}
                
                # Eƒüer tablonun baƒülamƒ±nƒ± korumak isteniyorsa
                if keep_table_context:
                    # Baƒülam ekle (tablonun bulunduƒüu yeri g√∂ster)
                    context_text = ""
                    chunks = pdf_data.get("chunks", [])
                    for chunk in chunks:
                        chunk_text = chunk.get("text", "")
                        # Eƒüer tabloya referans varsa
                        if f"Tablo {i+1}" in chunk_text or f"tablo {i+1}" in chunk_text.lower():
                            context_text += chunk_text + "\n\n"
                    
                    if context_text:
                        table_text += "\nBAƒûLAM:\n" + context_text
                
                # Create enhanced metadata with financial information
                table_metadata = {
                    "source": source,
                    "category": category,
                    "table_index": i,
                    "total_tables": len(pdf_data.get("tables", [])),
                    "document_title": pdf_data["metadata"].get("title", ""),
                    "type": "table",
                    "file_path": pdf_path,
                    "processing_time": pdf_data.get("processing_info", {}).get("processing_time_seconds", 0)
                }
                
                # Add financial metadata if available
                if financial_analysis:
                    table_metadata.update({
                        "financial_table_type": financial_analysis.get('table_type', 'general'),
                        "is_financial": financial_analysis.get('table_type') != 'financial_table',
                        "financial_years": financial_analysis.get('financial_data', {}).get('years', []),
                        "financial_currency": financial_analysis.get('financial_data', {}).get('currency'),
                        "financial_metrics": list(financial_analysis.get('financial_data', {}).get('metrics', {}).keys()),
                        "data_quality_score": financial_analysis.get('metadata', {}).get('data_quality_score', 0.0),
                        "financial_terms_found": financial_analysis.get('metadata', {}).get('financial_terms_found', [])
                    })
                
                documents.append({
                    "id": table_id,
                    "text": table_text,
                    "metadata": table_metadata
                })
        
        # Ana metin belgeleri
        for i, chunk in enumerate(pdf_data["chunks"]):
            document_id = f"{pdf_data['id']}_chunk_{i}"
            documents.append({
                "id": document_id,
                "text": chunk["text"],
                "metadata": {
                    "source": source,
                    "category": category,
                    "chunk_index": i,
                    "total_chunks": len(pdf_data["chunks"]),
                    "page_range": pdf_data["metadata"].get("total_pages", "unknown"),
                    "document_title": pdf_data["metadata"].get("title", ""),
                    "type": "text_chunk",
                    "chunk_size": chunk_size,
                    "file_path": pdf_path,
                    "processing_type": "gpu_ocr" if processor.use_gpu else "cpu_ocr" if use_ocr else "text_extraction",
                    "processing_time": pdf_data.get("processing_info", {}).get("processing_time_seconds", 0),
                    "ocr_engine": pdf_data.get("processing_info", {}).get("ocr_engine", "")
                }
            })
        
        # Tablolar √∂nceliklendirilmemi≈üse, ≈üimdi tablo belgelerini ekle
        if not prioritize_tables and extract_tables:
            # Tablo belgeleri - geli≈ütirilmi≈ü versiyon with financial processing
            for i, table in enumerate(pdf_data.get("tables", [])):
                table_id = f"{pdf_data['id']}_table_{i}"
                
                # Get original table dataframe for financial processing
                tables_dataframes = pdf_data.get("raw_tables", [])
                table_dataframe = tables_dataframes[i] if i < len(tables_dataframes) else None
                
                # Generate enhanced table text with financial analysis
                if table_dataframe is not None:
                    # Use financial processor for enhanced text generation
                    try:
                        # TEMPORARY FIX: Skip financial processor to avoid NaN issues
                        # financial_analysis = processor.financial_processor.process_financial_table(
                        #     table_dataframe, 
                        #     context=' '.join([chunk.get('text', '') for chunk in pdf_data.get('chunks', [])])
                        # )
                        # table_text = financial_analysis.get('text', '')
                        
                        # Use standard table processing instead
                        table_text = processor.tables_to_text([table_dataframe])
                        financial_analysis = {}
                        
                        # If financial text is empty, fallback to standard processing
                        if not table_text.strip():
                            table_text = processor.tables_to_text([table_dataframe])
                    except Exception as e:
                        logger.warning(f"Financial processing failed for table {i}: {str(e)}")
                        table_text = processor.tables_to_text([table_dataframe])
                        financial_analysis = {}
                else:
                    # Fallback: JSON formatƒ±ndan metin olu≈ütur
                    table_text = f"TABLO {i+1}:\n"
                    
                    # Tablo ba≈ülƒ±klarƒ±
                    headers = table.get("headers", [])
                    if headers:
                        table_text += " | ".join(str(h) for h in headers) + "\n"
                        table_text += "-" * (len(" | ".join(str(h) for h in headers))) + "\n"
                    
                    # Tablo verileri
                    for row in table.get("data", []):
                        if isinstance(row, dict):
                            row_values = [str(cell) if cell is not None else "" for cell in row.values()]
                        else:
                            row_values = [str(cell) if cell is not None else "" for cell in row]
                        table_text += " | ".join(row_values) + "\n"
                    
                    financial_analysis = {}
                
                # Eƒüer tablonun baƒülamƒ±nƒ± korumak isteniyorsa
                if keep_table_context:
                    # Baƒülam ekle (tablonun bulunduƒüu yeri g√∂ster)
                    context_text = ""
                    chunks = pdf_data.get("chunks", [])
                    for chunk in chunks:
                        chunk_text = chunk.get("text", "")
                        # Eƒüer tabloya referans varsa
                        if f"Tablo {i+1}" in chunk_text or f"tablo {i+1}" in chunk_text.lower():
                            context_text += chunk_text + "\n\n"
                    
                    if context_text:
                        table_text += "\nBAƒûLAM:\n" + context_text
                
                # Create enhanced metadata with financial information
                table_metadata = {
                    "source": source,
                    "category": category,
                    "table_index": i,
                    "total_tables": len(pdf_data.get("tables", [])),
                    "document_title": pdf_data["metadata"].get("title", ""),
                    "type": "table",
                    "file_path": pdf_path,
                    "processing_time": pdf_data.get("processing_info", {}).get("processing_time_seconds", 0)
                }
                
                # Add financial metadata if available
                if financial_analysis:
                    table_metadata.update({
                        "financial_table_type": financial_analysis.get('table_type', 'general'),
                        "is_financial": financial_analysis.get('table_type') != 'financial_table',
                        "financial_years": financial_analysis.get('financial_data', {}).get('years', []),
                        "financial_currency": financial_analysis.get('financial_data', {}).get('currency'),
                        "financial_metrics": list(financial_analysis.get('financial_data', {}).get('metrics', {}).keys()),
                        "data_quality_score": financial_analysis.get('metadata', {}).get('data_quality_score', 0.0),
                        "financial_terms_found": financial_analysis.get('metadata', {}).get('financial_terms_found', [])
                    })
                
                documents.append({
                    "id": table_id,
                    "text": table_text,
                    "metadata": table_metadata
                })
        
        # ƒ∞≈üleme durumunu logla
        processing_info = pdf_data.get("processing_info", {})
        if processing_info:
            logger.info(
                f"PDF i≈üleme tamamlandƒ±: {pdf_path}, "
                f"S√ºre: {processing_info.get('processing_time_seconds', 0):.2f} saniye, "
                f"OCR: {processing_info.get('ocr_used', False)}, "
                f"GPU: {processing_info.get('use_gpu', False)}, "
                f"Belge sayƒ±sƒ±: {len(documents)}"
            )
        
        return documents
    
    @staticmethod
    def process_pdf_directory_to_documents(
        directory: str,
        category_mapping: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        Bir dizindeki t√ºm PDF'leri doƒürudan belge listesine d√∂n√º≈üt√ºr√ºr.
        
        Args:
            directory: PDF dosyalarƒ±nƒ±n bulunduƒüu dizin
            category_mapping: Dosya adƒ± deseni -> kategori e≈üle≈ütirmeleri
            **kwargs: process_pdf_to_documents'e ge√ßirilecek ek parametreler
            
        Returns:
            ƒ∞≈ülenmi≈ü belgelerin listesi
        """
        documents = []
        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]
        
        for pdf_file in sorted(pdf_files):
            pdf_path = os.path.join(directory, pdf_file)
            
            # Kategoriyi belirle
            category = "genel"
            if category_mapping:
                for pattern, cat in category_mapping.items():
                    if pattern.lower() in pdf_file.lower():
                        category = cat
                        break
            
            try:
                # PDF'i belgelere d√∂n√º≈üt√ºr
                pdf_documents = EnhancedPdfProcessor.process_pdf_to_documents(
                    pdf_path,
                    category=category,
                    source=pdf_file,
                    **kwargs
                )
                documents.extend(pdf_documents)
            except Exception as e:
                logger.error(f"PDF i≈ülenirken hata: {pdf_file}, Hata: {str(e)}")
                continue
        
        logger.info(f"{len(pdf_files)} PDF dosyasƒ±ndan {len(documents)} belge olu≈üturuldu.")
        return documents
    
    def extract_text_with_pymupdf_streaming(self, pdf_path: str) -> Generator[str, None, None]:
        """
        PDF'ten PyMuPDF (fitz) kullanarak metin √ßƒ±karƒ±r (akƒ±≈ü tabanlƒ±).
        Her sayfa ayrƒ± ayrƒ± i≈ülenir ve bellek verimliliƒüi i√ßin yield edilir.
        
        Args:
            pdf_path: PDF dosya yolu
            
        Yields:
            Her sayfadan √ßƒ±karƒ±lan metin
        """
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(doc.page_count):
                # Bellek kullanƒ±mƒ±nƒ± kontrol et
                self._check_memory_usage()
                
                # Sayfayƒ± y√ºkle ve metni √ßƒ±kar
                page = doc.load_page(page_num)
                text = page.get_text()
                
                # Sayfayƒ± temizle
                page = None
                
                # Sayfadan √ßƒ±karƒ±lan metni yield et
                yield text
                
                # Her 10 sayfada bir belleƒüi temizle
                if page_num % 10 == 0:
                    gc.collect()
            
            # D√∂k√ºmanƒ± kapat ve belleƒüi temizle
            doc.close()
            doc = None
            gc.collect()
            
        except Exception as e:
            logger.error(f"PyMuPDF ile metin √ßƒ±karƒ±rken hata: {str(e)}")
            yield ""
    
    def extract_text_with_pdfplumber_streaming(self, pdf_path: str) -> Generator[str, None, None]:
        """
        PDF'ten pdfplumber kullanarak metin √ßƒ±karƒ±r (akƒ±≈ü tabanlƒ±).
        Her sayfa ayrƒ± ayrƒ± i≈ülenir ve bellek verimliliƒüi i√ßin yield edilir.
        
        Args:
            pdf_path: PDF dosya yolu
            
        Yields:
            Her sayfadan √ßƒ±karƒ±lan metin
        """
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    # Bellek kullanƒ±mƒ±nƒ± kontrol et
                    self._check_memory_usage()
                    
                    # Metni √ßƒ±kar
                    text = page.extract_text() or ""
                    
                    # Sayfadan √ßƒ±karƒ±lan metni yield et
                    yield text
                    
                    # Her 10 sayfada bir belleƒüi temizle
                    if i % 10 == 0:
                        gc.collect()
        except Exception as e:
            logger.error(f"pdfplumber ile metin √ßƒ±karƒ±rken hata: {str(e)}")
            yield ""
    
    def process_pdf_streaming(
        self, 
        pdf_path: str, 
        category: str = "genel",
        output_path: Optional[str] = None,
        chunk_size: int = 1000,
        overlap: int = 200
    ) -> Dict[str, Any]:
        """
        Bir PDF dosyasƒ±nƒ± akƒ±≈ü tabanlƒ± y√∂ntemle i≈üler.
        Bu y√∂ntem b√ºy√ºk PDF dosyalarƒ± i√ßin bellek verimlidir.
        
        Args:
            pdf_path: ƒ∞≈ülenecek PDF dosya yolu
            category: Belge kategorisi
            output_path: √áƒ±ktƒ± dosya yolu (None ise otomatik olu≈üturulur)
            chunk_size: Metin par√ßalama uzunluƒüu
            overlap: Metin par√ßalarƒ± arasƒ± √∂rt√º≈üme
            
        Returns:
            ƒ∞≈ülenmi≈ü PDF verileri √∂zeti
        """
        if not self._validate_pdf_file(pdf_path):
            logger.error(f"Ge√ßersiz PDF dosyasƒ±: {pdf_path}")
            return {}
        
        # Temel metadata olu≈ütur
        metadata = self.extract_metadata(pdf_path)
        
        # Sonu√ß s√∂zl√ºƒü√ºn√º ba≈ülat
        result = {
            "id": os.path.splitext(os.path.basename(pdf_path))[0],
            "source": os.path.basename(pdf_path),
            "category": category,
            "metadata": metadata,
            "chunk_count": 0,
            "total_words": 0,
            "processing_completed": False
        }
        
        # √áƒ±ktƒ± yolunu belirle
        if not output_path:
            output_path = self.output_dir / f"{result['id']}.json"
        else:
            output_path = Path(output_path)
        
        try:
            # Akƒ±≈ü i≈ülemcisini ba≈ülat
            text_processor = TextStreamProcessor(
                output_path=output_path, 
                chunk_size=chunk_size, 
                overlap=overlap,
                cleanup_funcs=self.text_cleaners
            )
            
            # Akƒ±≈ü i≈üleme i√ßin sayfa sayfa oku
            page_count = 0
            word_count = 0
            
            # PyMuPDF ile metni √ßƒ±kar
            for page_text in self.extract_text_with_pymupdf_streaming(pdf_path):
                if page_text.strip():
                    # Metni temizle ve i≈üle
                    cleaned_text = self.clean_text(page_text)
                    chunks = text_processor.process_text_chunk(cleaned_text)
                    
                    page_count += 1
                    word_count += len(cleaned_text.split())
                
                # D√ºzenli bellek temizliƒüi
                if page_count % 5 == 0:
                    self._check_memory_usage()
            
            # PyMuPDF ile metin bulunamazsa pdfplumber dene
            if page_count == 0:
                for page_text in self.extract_text_with_pdfplumber_streaming(pdf_path):
                    if page_text.strip():
                        # Metni temizle ve i≈üle
                        cleaned_text = self.clean_text(page_text)
                        chunks = text_processor.process_text_chunk(cleaned_text)
                        
                        page_count += 1
                        word_count += len(cleaned_text.split())
                    
                    # D√ºzenli bellek temizliƒüi
                    if page_count % 5 == 0:
                        self._check_memory_usage()
            
            # OCR i≈ülemi akƒ±≈ü tabanlƒ± deƒüil, bu y√ºzden sadece metin bulunamazsa uygula
            if page_count == 0 and self.ocr_available:
                logger.info(f"Metinsel i√ßerik bulunamadƒ±, OCR uygulanƒ±yor: {pdf_path}")
                ocr_text = self.perform_ocr(pdf_path)
                if ocr_text.strip():
                    cleaned_text = self.clean_text(ocr_text)
                    chunks = text_processor.process_text_chunk(cleaned_text)
                    word_count = len(cleaned_text.split())
            
            # ƒ∞≈ülemeyi tamamla
            chunk_count = text_processor.finalize()
            
            # Sonu√ß bilgilerini g√ºncelle
            result["chunk_count"] = chunk_count
            result["total_words"] = word_count
            result["processing_completed"] = True
            
            # √ñzet bilgileri kaydet
            with open(str(output_path).replace('.json', '_summary.json'), 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"PDF akƒ±≈ü tabanlƒ± i≈üleme tamamlandƒ±: {pdf_path}")
            return result
            
        except Exception as e:
            logger.error(f"PDF akƒ±≈ü tabanlƒ± i≈üleme hatasƒ±: {pdf_path}, Hata: {str(e)}")
            result["error"] = str(e)
            return result
    
    @force_gc_after
    def process_pdf_directory_streaming(
        self, 
        directory: str, 
        category_mapping: Optional[Dict[str, str]] = None,
        parallel: bool = True,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        Bir dizindeki t√ºm PDF dosyalarƒ±nƒ± akƒ±≈ü tabanlƒ± y√∂ntemle i≈üler.
        
        Args:
            directory: PDF dosyalarƒ±nƒ±n bulunduƒüu dizin
            category_mapping: Dosya adƒ± deseni -> kategori e≈üle≈ütirmeleri
            parallel: Paralel i≈üleme kullanƒ±lsƒ±n mƒ±
            **kwargs: process_pdf_streaming'e ge√ßirilecek ek parametreler
            
        Returns:
            ƒ∞≈ülenmi≈ü PDF verilerinin listesi
        """
        results = []
        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]
        
        if parallel and len(pdf_files) > 1:
            # Paralel i≈üleme
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                
                for pdf_file in sorted(pdf_files):
                    pdf_path = os.path.join(directory, pdf_file)
                    
                    # Kategoriyi belirle
                    category = "genel"
                    if category_mapping:
                        for pattern, cat in category_mapping.items():
                            if pattern.lower() in pdf_file.lower():
                                category = cat
                                break
                    
                    # ƒ∞≈üleme g√∂revi ekle
                    future = executor.submit(
                        self.process_pdf_streaming,
                        pdf_path=pdf_path,
                        category=category,
                        **kwargs
                    )
                    futures.append(future)
                
                # T√ºm g√∂revlerin tamamlanmasƒ±nƒ± bekle
                for future in futures:
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                    except Exception as e:
                        logger.error(f"Paralel PDF i≈üleme hatasƒ±: {str(e)}")
        else:
            # Sƒ±ralƒ± i≈üleme
            for pdf_file in sorted(pdf_files):
                pdf_path = os.path.join(directory, pdf_file)
                
                # Kategoriyi belirle
                category = "genel"
                if category_mapping:
                    for pattern, cat in category_mapping.items():
                        if pattern.lower() in pdf_file.lower():
                            category = cat
                            break
                
                try:
                    # PDF'i i≈üle
                    result = self.process_pdf_streaming(
                        pdf_path=pdf_path,
                        category=category,
                        **kwargs
                    )
                    if result:
                        results.append(result)
                except Exception as e:
                    logger.error(f"PDF i≈ülenirken hata: {pdf_file}, Hata: {str(e)}")
                    continue
                
                # Bellek temizliƒüi
                self._check_memory_usage()
        
        logger.info(f"{len(results)} PDF dosyasƒ± akƒ±≈ü tabanlƒ± i≈üleme ile ba≈üarƒ±yla i≈ülendi.")
        return results 